{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://vnexpress.net/suc-khoe-p1\n",
      "Scraping: https://vnexpress.net/suc-khoe-p2\n",
      "Scraping: https://vnexpress.net/suc-khoe-p3\n",
      "Scraping: https://vnexpress.net/suc-khoe-p4\n",
      "Scraping: https://vnexpress.net/suc-khoe-p5\n",
      "Scraping: https://vnexpress.net/suc-khoe-p6\n",
      "Scraping: https://vnexpress.net/suc-khoe-p7\n",
      "Scraping: https://vnexpress.net/suc-khoe-p8\n",
      "Scraping: https://vnexpress.net/suc-khoe-p9\n",
      "Scraping: https://vnexpress.net/suc-khoe-p10\n",
      "Scraping: https://vnexpress.net/suc-khoe-p11\n",
      "Scraping: https://vnexpress.net/suc-khoe-p12\n",
      "Scraping: https://vnexpress.net/suc-khoe-p13\n",
      "Scraping: https://vnexpress.net/suc-khoe-p14\n",
      "Scraping: https://vnexpress.net/suc-khoe-p15\n",
      "Scraping: https://vnexpress.net/suc-khoe-p16\n",
      "Scraping: https://vnexpress.net/suc-khoe-p17\n",
      "Scraping: https://vnexpress.net/suc-khoe-p18\n",
      "Scraping: https://vnexpress.net/suc-khoe-p19\n",
      "Scraping: https://vnexpress.net/suc-khoe-p20\n",
      "✅ Đã lưu thành công vào file CSV!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_vnexpress_health(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        article_links = soup.find_all('h3', class_='title-news')\n",
    "        \n",
    "        for link in article_links:\n",
    "            article_url = link.a['href']\n",
    "            article = scrape_article(article_url)\n",
    "            if article:\n",
    "                articles.append(article)\n",
    "        \n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            title = soup.find('h1', class_='title-detail').text.strip()\n",
    "        except:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            keywords = soup.find('meta', attrs={'name': 'keywords'})['content']\n",
    "        except:\n",
    "            keywords = \"\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', attrs={'name': 'description'})['content']\n",
    "        except:\n",
    "            description = \"\"\n",
    "\n",
    "        try:\n",
    "            content = ' '.join(p.text.strip() for p in soup.find_all('p', class_='Normal'))\n",
    "        except:\n",
    "            content = \"\"\n",
    "\n",
    "        content_length = len(content)\n",
    "\n",
    "        author_tag = soup.find('p', class_='author')\n",
    "        author = author_tag.text.strip() if author_tag else \"\"\n",
    "\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'keywords': keywords,\n",
    "            'description': description,\n",
    "            'content': content,\n",
    "            'len': content_length,\n",
    "            'author': author\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to retrieve article {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "def main():\n",
    "    base_url = 'https://vnexpress.net/suc-khoe-p'\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, 21):  \n",
    "        page_url = f'{base_url}{page}'\n",
    "        print(f\"Scraping: {page_url}\")\n",
    "        articles = scrape_vnexpress_health(page_url)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if all_articles:\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        df.to_csv('vnexpress_health_articles.csv', index=False, encoding='utf-8-sig')\n",
    "        print('✅ Đã lưu thành công vào file CSV!')\n",
    "    else:\n",
    "        print('⚠️ Không có bài viết nào được lấy.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
